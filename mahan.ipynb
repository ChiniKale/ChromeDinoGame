{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "say it with me: who is great?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dinosaur import Dinosaur  # Assuming this contains the Dinosaur class with image rendering\n",
    "from obstacle import Obstacle  # Assuming this contains the Obstacle class with image rendering\n",
    "from batsymbol import Batsymb  # Assuming this contains BatSymbol for power-ups or extra obstacles\n",
    "\n",
    "# Screen dimensions\n",
    "SCREEN_WIDTH = 800\n",
    "SCREEN_HEIGHT = 400\n",
    "\n",
    "# Game constants\n",
    "VELOCITY = 5\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Screen setup\n",
    "screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# DQN model definition\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Get the game state\n",
    "def get_game_state(dinosaur, obstacles, velocity):\n",
    "    closest_obstacle = min(obstacles, key=lambda obs: obs.get_distance_to(dinosaur))\n",
    "    dino_rect = dinosaur.get_rect()\n",
    "    state = [\n",
    "        dino_rect.x,  # Dinosaur's x-position\n",
    "        dino_rect.y,  # Dinosaur's y-position\n",
    "        closest_obstacle.get_distance_to(dinosaur),  # Distance to the closest obstacle\n",
    "        velocity,  # Current velocity\n",
    "    ]\n",
    "    return state\n",
    "\n",
    "# Initialize models and training components\n",
    "state_dim = 4  # dino_x, dino_y, obstacle_distance, velocity\n",
    "action_dim = 2  # Actions: [Do nothing, Jump]\n",
    "model = DQN(state_dim, action_dim)\n",
    "target_model = DQN(state_dim, action_dim)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Replay memory\n",
    "memory = deque(maxlen=2000)\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dinosaur.__init__() missing 1 required positional argument: 'surfaceHeight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Game loop and training integration\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):  \u001b[38;5;66;03m# Number of episodes\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     dinosaur \u001b[38;5;241m=\u001b[39m \u001b[43mDinosaur\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Instantiate Dinosaur from your file\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     obstacles \u001b[38;5;241m=\u001b[39m [Obstacle()]  \u001b[38;5;66;03m# Instantiate Obstacles\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Dinosaur.__init__() missing 1 required positional argument: 'surfaceHeight'"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_dqn():\n",
    "    if len(memory) < 32:\n",
    "        return\n",
    "    batch = random.sample(memory, 32)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int64)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "    q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    next_q_values = target_model(next_states).max(1)[0]\n",
    "    target_q_values = rewards + (1 - dones) * gamma * next_q_values\n",
    "\n",
    "    loss = criterion(q_values, target_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Game loop and training integration\n",
    "for episode in range(500):  # Number of episodes\n",
    "    dinosaur = Dinosaur()  # Instantiate Dinosaur from your file\n",
    "    obstacles = [Obstacle()]  # Instantiate Obstacles\n",
    "    total_reward = 0\n",
    "    running = True\n",
    "\n",
    "    while running:\n",
    "        screen.fill((255, 255, 255))  # White background\n",
    "\n",
    "        # Game events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "                pygame.quit()\n",
    "\n",
    "        # Get current state\n",
    "        state = get_game_state(dinosaur, obstacles, VELOCITY)\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = random.choice([0, 1])  # Random action\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "                q_values = model(state_tensor)\n",
    "                action = torch.argmax(q_values).item()\n",
    "\n",
    "        # Perform action\n",
    "        if action == 1:  # Jump\n",
    "            dinosaur.jump()\n",
    "\n",
    "        # Update game entities\n",
    "        dinosaur.update()\n",
    "        for obstacle in obstacles:\n",
    "            obstacle.update()\n",
    "            if obstacle.is_off_screen():\n",
    "                obstacles.remove(obstacle)\n",
    "                obstacles.append(Obstacle())  # Add a new obstacle\n",
    "\n",
    "        # Check for collisions\n",
    "        reward = 1  # Reward for survival\n",
    "        if any(obstacle.collides_with(dinosaur) for obstacle in obstacles):\n",
    "            reward = -10  # Penalty for collision\n",
    "            running = False\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        # Update state\n",
    "        next_state = get_game_state(dinosaur, obstacles, VELOCITY)\n",
    "        done = not running\n",
    "\n",
    "        # Store transition in memory\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Train the model\n",
    "        train_dqn()\n",
    "\n",
    "        # Render game objects\n",
    "        dinosaur.draw(screen)\n",
    "        for obstacle in obstacles:\n",
    "            obstacle.draw(screen)\n",
    "\n",
    "        pygame.display.flip()\n",
    "        clock.tick(30)\n",
    "\n",
    "    # Update epsilon and target network\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "    if episode % 10 == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'game_display' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 143\u001b[0m\n\u001b[1;32m    140\u001b[0m     dinosaur\u001b[38;5;241m.\u001b[39mduck(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Simulate the game and get reward\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m \u001b[43mgame_display\u001b[49m\u001b[38;5;241m.\u001b[39mfill((\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m))  \u001b[38;5;66;03m# White background\u001b[39;00m\n\u001b[1;32m    144\u001b[0m ground_scroll \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m VELOCITY \u001b[38;5;241m*\u001b[39m delta_time\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ground_scroll \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mground_width:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'game_display' is not defined"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from pygame import mixer\n",
    "from dinosaur import Dinosaur\n",
    "from obstacle import Obstacle\n",
    "from batsymbol import Batsymb\n",
    "\n",
    "# Initialize pygame and set up game window\n",
    "pygame.init()\n",
    "size = width, height = 640, 480\n",
    "gameDisplay = pygame.display.set_mode(size)\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# Initialize background music\n",
    "mixer.init()\n",
    "mixer.music.load(\"bgm.mp3\")\n",
    "mixer.music.set_volume(0.7)\n",
    "mixer.music.play(loops=-1)\n",
    "\n",
    "# Initialize game variables\n",
    "ground_height = height - 100\n",
    "game_timer = 0\n",
    "collision = False\n",
    "dinosaur = Dinosaur(ground_height)\n",
    "last_frame = pygame.time.get_ticks()\n",
    "MINGAP = 200\n",
    "MAXGAP = 600\n",
    "MAXSIZE = 40\n",
    "MINSIZE = 20\n",
    "obstacles = []\n",
    "last_obstacle = width\n",
    "ground_image = pygame.image.load(r\"ground.png\")\n",
    "ground_width = ground_image.get_width()\n",
    "ground_scroll = 0\n",
    "\n",
    "# Initialize DQN parameters\n",
    "ACTION_SPACE = 3  # [0: duck, 1: jump, 2: do nothing]\n",
    "STATE_SIZE = 4  # Example state size, you can increase the state size based on the game\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "EPSILON_MIN = 0.01\n",
    "TRAINING_START = 1000\n",
    "MEMORY_SIZE = 2000\n",
    "\n",
    "# Experience replay buffer\n",
    "memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Define the neural network for DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(STATE_SIZE, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, ACTION_SPACE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the agent and the model\n",
    "model = DQN()\n",
    "target_model = DQN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Copy weights from model to target_model\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "target_model.eval()\n",
    "\n",
    "# Define function to select action based on epsilon-greedy policy\n",
    "def select_action(state):\n",
    "    if random.random() < EPSILON:\n",
    "        return random.randint(0, ACTION_SPACE - 1)\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    q_values = model(state)\n",
    "    return torch.argmax(q_values).item()\n",
    "\n",
    "# Define function to store experience in the replay buffer\n",
    "def store_experience(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "# Define function to train the DQN\n",
    "def train_dqn():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(memory, BATCH_SIZE)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = torch.FloatTensor(states)\n",
    "    next_states = torch.FloatTensor(next_states)\n",
    "    actions = torch.LongTensor(actions)\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    dones = torch.FloatTensor(dones)\n",
    "\n",
    "    # Get Q-values for current states and actions\n",
    "    q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Get Q-values for next states from target model\n",
    "    next_q_values = target_model(next_states).max(1)[0]\n",
    "    target_q_values = rewards + (GAMMA * next_q_values * (1 - dones))\n",
    "\n",
    "    # Calculate loss and optimize\n",
    "    loss = nn.MSELoss()(q_values, target_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update epsilon for exploration\n",
    "    global EPSILON\n",
    "    EPSILON = max(EPSILON * EPSILON_DECAY, EPSILON_MIN)\n",
    "\n",
    "# Game loop\n",
    "while True:\n",
    "    t = pygame.time.get_ticks()\n",
    "    delta_time = (t - last_frame) / 1000.0\n",
    "    last_frame = t\n",
    "\n",
    "    # Get the current state of the game\n",
    "    state = np.array([dinosaur.x, dinosaur.y, len(obstacles), ground_scroll])\n",
    "\n",
    "    # Select action using the DQN\n",
    "    action = select_action(state)\n",
    "\n",
    "    # Perform action\n",
    "    if action == 0:  # Duck\n",
    "        dinosaur.duck(True)\n",
    "    elif action == 1:  # Jump\n",
    "        dinosaur.bigjump()\n",
    "    else:  # Do nothing\n",
    "        dinosaur.duck(False)\n",
    "\n",
    "    # Simulate the game and get reward\n",
    "    game_display.fill((255, 255, 255))  # White background\n",
    "    ground_scroll -= VELOCITY * delta_time\n",
    "    if ground_scroll <= -ground_width:\n",
    "        ground_scroll += ground_width\n",
    "    game_display.blit(ground_image, (ground_scroll, 300))\n",
    "\n",
    "    draw_text(f\"Score: {int(game_timer * 10)}\", text_font, (0, 255, 0), 100, 50)\n",
    "    dinosaur.update(delta_time)\n",
    "    dinosaur.draw(game_display)\n",
    "\n",
    "    # Add new obstacles\n",
    "    if len(obstacles) == 0 or obstacles[-1].x < width - MINGAP:\n",
    "        ahaha = random.random()\n",
    "        is_high = ahaha > 0.7\n",
    "        obstacle_size = random.randint(MINSIZE, MAXSIZE) if not is_high else 30\n",
    "        obstacles.append(Obstacle(last_obstacle, obstacle_size, ground_height, is_high))\n",
    "        last_obstacle += MINGAP + (MAXGAP - MINGAP) * random.random() + 0.01 * game_timer * 1000\n",
    "\n",
    "    # Handle obstacle movement and collision\n",
    "    reward = -1  # Default reward for each step (penalty)\n",
    "    for obs in obstacles:\n",
    "        obs.update(delta_time, VELOCITY)\n",
    "        obs.draw(game_display)\n",
    "\n",
    "        dino_rect = pygame.Rect(dinosaur.x, dinosaur.surfaceHeight - dinosaur.y - dinosaur.height, dinosaur.width, dinosaur.height)\n",
    "        obs_rect = pygame.Rect(obs.x, obs.y, obs.size, obs.size)\n",
    "\n",
    "        if dino_rect.colliderect(obs_rect):\n",
    "            reward = -10  # Large penalty for collision\n",
    "            done = True\n",
    "            break\n",
    "    else:\n",
    "        reward = 1  # Small reward for surviving\n",
    "        done = False\n",
    "\n",
    "    # Next state after taking action\n",
    "    next_state = np.array([dinosaur.x, dinosaur.y, len(obstacles), ground_scroll])\n",
    "\n",
    "    # Store experience in memory buffer\n",
    "    store_experience(state, action, reward, next_state, done)\n",
    "\n",
    "    # Train the DQN\n",
    "    train_dqn()\n",
    "\n",
    "    # Update the target model every few steps\n",
    "    if t % 100 == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    pygame.display.update()\n",
    "    clock.tick(60)  # Limit the frame rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
